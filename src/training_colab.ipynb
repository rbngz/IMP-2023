{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB NECESSARY CELL\n",
    "from google.colab import drive\n",
    "\n",
    "# Clone repository with all classes\n",
    "github_pat = input(\"Github Personal Access Token\")\n",
    "!git clone https://{github_pat}@github.com/rbngz/IMP-2023.git\n",
    "\n",
    "# Navigate to repository source\n",
    "%cd /content/IMP-2023/src\n",
    "\n",
    "# Mount drive folder\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install additional libraries\n",
    "!pip install -U wandb lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from dataset_new import SentinelDataset\n",
    "from utils import get_dataset_stats\n",
    "from transforms import TargetNormalize\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for splitting\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "# File paths\n",
    "SAMPLES_PATH = \"/content/drive/MyDrive/imp-data/samples/samples_S2S5P_2018_2020_eea.csv\"\n",
    "DATA_DIR  = \"/content/drive/MyDrive/imp-data/sentinel-2-eea\"\n",
    "\n",
    "# Hyperparameters\n",
    "N_PATCHES = 4\n",
    "PATCH_SIZE = 64\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the samples file\n",
    "samples_df = pd.read_csv(SAMPLES_PATH, index_col=\"idx\")\n",
    "\n",
    "# Remove NA measurements\n",
    "samples_df = samples_df[~samples_df[\"no2\"].isna()]\n",
    "\n",
    "# Random shuffle\n",
    "samples_df = samples_df.sample(frac=1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split samples dataframe to avoid sampling patches across sets\n",
    "df_train, df_val, df_test = np.split(\n",
    "    samples_df, [int(0.7 * len(samples_df)), int(0.85 * len(samples_df))]\n",
    ")\n",
    "print(f\"Train set: {len(df_train)}, Validation set: {len(df_val)}, Test set: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for normalization\n",
    "stats_train = get_dataset_stats(df_train, DATA_DIR)\n",
    "print(stats_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalizers for bands and NO2 measurements\n",
    "band_normalize = Normalize(stats_train[\"band_means\"], stats_train[\"band_stds\"])\n",
    "no2_normalize = TargetNormalize(stats_train[\"no2_mean\"], stats_train[\"no2_std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train Dataset\n",
    "dataset_train = SentinelDataset(\n",
    "    df_train,\n",
    "    DATA_DIR,\n",
    "    n_patches=N_PATCHES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    pre_load=False,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Create Validation Dataset\n",
    "dataset_val = SentinelDataset(\n",
    "    df_val,\n",
    "    DATA_DIR,\n",
    "    n_patches=N_PATCHES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    pre_load=False,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Create Test Dataset\n",
    "dataset_test = SentinelDataset(\n",
    "    df_test,\n",
    "    DATA_DIR,\n",
    "    n_patches=N_PATCHES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    pre_load=False,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train dataset: {len(dataset_train)}, Validation dataset: {len(dataset_val)}, Test dataset: {len(dataset_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, persistent_workers=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, num_workers=12, persistent_workers=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pytorch lightning model\n",
    "\n",
    "class Model(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set model\n",
    "        self.model = UNet(\n",
    "            enc_chs=(12, 64, 128, 256, 512),\n",
    "            dec_chs=(512, 256, 128, 64)\n",
    "            )\n",
    "\n",
    "        # Set hyperparameters\n",
    "        self.loss = MSELoss()\n",
    "        self.mae = L1Loss()\n",
    "        self.n_patches = N_PATCHES\n",
    "        self.patch_size = PATCH_SIZE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.lr = LEARNING_RATE\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, mae = self._step(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_mae\", mae)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, mae = self._step(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_mae\", mae)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, mae = self._step(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_mae\", mae)\n",
    "        return loss\n",
    "    \n",
    "    def _step(self, batch):\n",
    "        # Unpack batch\n",
    "        patches, measurements, coords = batch\n",
    "\n",
    "        # Normalize band and measurement data\n",
    "        patches_norm = band_normalize(patches)\n",
    "        measurements_norm = no2_normalize(measurements)\n",
    "\n",
    "        # Get normalized predictions\n",
    "        predictions_norm = self.model(patches_norm)\n",
    "\n",
    "        # Extract values in coordinate location\n",
    "        target_values_norm = torch.diag(predictions_norm[:, 0, coords[0], coords[1]])\n",
    "\n",
    "        # Compute loss on normalized data\n",
    "        loss = self.loss(target_values_norm, measurements_norm)\n",
    "\n",
    "        # Compute Mean Absolute Error on unnormalized data\n",
    "        target_values = no2_normalize.revert(target_values_norm)\n",
    "        mae = self.mae(target_values, measurements)\n",
    "\n",
    "        return loss, mae\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logger for weights & biases\n",
    "wandb_logger = WandbLogger(project=\"IMP-2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer = L.Trainer(max_epochs=100, logger=wandb_logger, log_every_n_steps=400)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

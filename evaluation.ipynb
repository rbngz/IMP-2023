{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "\n",
    "from torchvision.transforms.v2 import ToImage, Compose\n",
    "from torch.nn import MSELoss, L1Loss, CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval.metrics import R2Score\n",
    "\n",
    "from core.dataset import SentinelDataset\n",
    "from core.model import UNet\n",
    "from core.utils import normalize_rgb_bands\n",
    "from core.transforms import BandNormalize, TargetNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DATA_DIR = \"data\"\n",
    "DATA_SOURCE = \"eea\"\n",
    "SKIP_CONNECTIONS = False\n",
    "LAND_COVER = False\n",
    "CHECKPOINT = \"models/unet_ae_s2s5p/ae_no2.ckpt\"\n",
    "PRED_SIZE = 8\n",
    "PATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed for splitting\n",
    "SEED = 42\n",
    "L.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_PATH = os.path.join(\n",
    "    DATA_DIR, f\"samples/samples_S2S5P_2018_2020_{DATA_SOURCE}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.read_csv(SAMPLES_PATH, index_col=\"idx\")\n",
    "# Remove NA measurements\n",
    "samples_df = samples_df[~samples_df[\"no2\"].isna()]\n",
    "\n",
    "# Exclude samples for which no valid land cover ground truth is present ~200\n",
    "if DATA_SOURCE== \"eea\":\n",
    "    valid_land_cover_stations = []\n",
    "    land_cover_path = os.path.join(DATA_DIR, \"worldcover\")\n",
    "    for file in os.listdir(land_cover_path):\n",
    "        lc = np.load(os.path.join(land_cover_path, file))\n",
    "        if lc.shape == (200, 200):\n",
    "            valid_land_cover_stations.append(file[:-4])\n",
    "\n",
    "    samples_df = samples_df.loc[\n",
    "        samples_df[\"AirQualityStation\"].isin(valid_land_cover_stations)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubengonzalez/miniconda3/envs/imp/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Random shuffle\n",
    "samples_df = samples_df.sample(frac=1)\n",
    "# Split samples dataframe to avoid sampling patches across sets\n",
    "if DATA_SOURCE== \"eea\":\n",
    "    df_train, df_val, df_test = np.split(\n",
    "        samples_df, [int(0.7 * len(samples_df)), int(0.85 * len(samples_df))]\n",
    "    )\n",
    "else:\n",
    "    df_test = samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_train = {\n",
    "    \"band_means\": np.array(\n",
    "        [\n",
    "            9.48530855e02,\n",
    "            8.85735776e02,\n",
    "            6.69150641e02,\n",
    "            2.31917082e03,\n",
    "            1.28305729e03,\n",
    "            1.97936703e03,\n",
    "            2.23097768e03,\n",
    "            2.38542771e03,\n",
    "            2.06128535e03,\n",
    "            1.55304775e03,\n",
    "            5.61707384e02,\n",
    "            2.38793057e03,\n",
    "            2.78282474e15,\n",
    "        ]\n",
    "    ),\n",
    "    \"band_stds\": np.array(\n",
    "        [\n",
    "            6.69703046e02,\n",
    "            5.34104387e02,\n",
    "            4.92931981e02,\n",
    "            1.00871675e03,\n",
    "            5.90429095e02,\n",
    "            7.41831336e02,\n",
    "            8.81957446e02,\n",
    "            9.48550975e02,\n",
    "            8.09822953e02,\n",
    "            7.57415252e02,\n",
    "            3.27955892e02,\n",
    "            8.74079961e02,\n",
    "            1.36616750e15,\n",
    "        ]\n",
    "    ),\n",
    "    \"no2_mean\": 20.973578214241755,\n",
    "    \"no2_std\": 11.575741710970245,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create normalizers for bands and NO2 measurements\n",
    "band_normalize = BandNormalize(stats_train[\"band_means\"], stats_train[\"band_stds\"])\n",
    "no2_normalize = TargetNormalize(stats_train[\"no2_mean\"], stats_train[\"no2_std\"])\n",
    "\n",
    "# Create transforms for images and measurements\n",
    "s2_transform = Compose([ToImage(), band_normalize])\n",
    "no2_transform = no2_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Dataset\n",
    "dataset_test = SentinelDataset(\n",
    "    samples_df,\n",
    "    DATA_DIR,\n",
    "    n_patches=1,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    pred_size=PRED_SIZE,\n",
    "    pre_load=False,\n",
    "    s2_transform=s2_transform,\n",
    "    no2_transform=no2_transform,\n",
    "    data_source=DATA_SOURCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(dataset_test, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [\n",
    "    1.88037399e02,\n",
    "    1.64854777e00,\n",
    "    5.85467415e01,\n",
    "    3.36591268e00,\n",
    "    6.51197767e00,\n",
    "    1.00000000e00,\n",
    "    9.56108398e01,\n",
    "    3.37424575e06,\n",
    "    1.38434725e01,\n",
    "    3.68938629e02,\n",
    "    2.18333547e05,\n",
    "]\n",
    "\n",
    "\n",
    "# Define Pytorch lightning model\n",
    "class Model(L.LightningModule):\n",
    "    def __init__(self, model, lr, include_lc, lc_loss_weight, lc_class_weights):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set model\n",
    "        self.model = model\n",
    "\n",
    "        # Set hyperparameters\n",
    "        self.no2_loss = MSELoss()\n",
    "        self.no2_mae = L1Loss()\n",
    "        self.lc_loss = CrossEntropyLoss(weight=torch.tensor(lc_class_weights))\n",
    "        self.include_lc = include_lc\n",
    "        self.lc_loss_weight = lc_loss_weight\n",
    "        self.lr = lr\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        no2_loss, no2_mae, lc_loss = self._step(batch)\n",
    "        total_loss = no2_loss + (self.lc_loss_weight * lc_loss)\n",
    "        self.log(\"train_no2_loss\", no2_loss)\n",
    "        self.log(\"train_no2_mae\", no2_mae)\n",
    "        self.log(\"train_lc_loss\", lc_loss)\n",
    "        self.log(\"train_total_loss\", total_loss)\n",
    "        loss = total_loss if self.include_lc else no2_loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        no2_loss, no2_mae, lc_loss = self._step(batch, batch_idx == 0)\n",
    "        total_loss = no2_loss + (self.lc_loss_weight * lc_loss)\n",
    "        self.log(\"val_no2_loss\", no2_loss)\n",
    "        self.log(\"val_no2_mae\", no2_mae)\n",
    "        self.log(\"val_lc_loss\", lc_loss)\n",
    "        self.log(\"val_total_loss\", total_loss)\n",
    "        loss = total_loss if self.include_lc else no2_loss\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        no2_loss, no2_mae, lc_loss, no2_mse, r2_score = self._step(batch,test=True)\n",
    "        total_loss = no2_loss + (self.lc_loss_weight * lc_loss)\n",
    "        self.log(\"test_no2_loss\", no2_loss)\n",
    "        self.log(\"test_no2_mae\", no2_mae)\n",
    "        self.log(\"test_no2_mse\", no2_mse)\n",
    "        self.log(\"test_no2_r2\", r2_score)\n",
    "        self.log(\"test_lc_loss\", lc_loss)\n",
    "        self.log(\"test_total_loss\", total_loss)\n",
    "        loss = total_loss if self.include_lc else no2_loss\n",
    "        return loss\n",
    "\n",
    "    def _step(self, batch, log_predictions=False, test=False):\n",
    "        # Unpack batch\n",
    "        patches_norm, lc_truth, measurements_norm, coords = batch\n",
    "\n",
    "        # Get normalized predictions\n",
    "        predictions_norm, land_cover_pred = self.model(patches_norm)\n",
    "\n",
    "        # Extract values in coordinate location\n",
    "        target_values_norm = torch.diag(predictions_norm[:, 0, coords[0], coords[1]])\n",
    "\n",
    "        # Compute loss on normalized data\n",
    "        no2_loss = self.no2_loss(target_values_norm, measurements_norm)\n",
    "\n",
    "        # Center crop\n",
    "        # lc_truth = CenterCrop(land_cover_pred.shape[-2:])(lc_truth)\n",
    "        lc_loss = self.lc_loss(land_cover_pred, lc_truth)\n",
    "\n",
    "        # Compute Mean Absolute Error on unnormalized data\n",
    "        measurements = no2_normalize.revert(measurements_norm)\n",
    "        target_values = no2_normalize.revert(target_values_norm)\n",
    "        no2_mae = self.no2_mae(target_values, measurements)\n",
    "\n",
    "        if test:\n",
    "            no2_mse = self.no2_loss(target_values, measurements)\n",
    "            metric = R2Score()\n",
    "            metric.update(target_values, measurements)\n",
    "            r2_score = metric.compute()\n",
    "            return no2_loss, no2_mae, lc_loss, no2_mse, r2_score\n",
    "\n",
    "        if log_predictions:\n",
    "            self.logger.log_image(\n",
    "                \"images\",\n",
    "                [\n",
    "                    torch.moveaxis(normalize_rgb_bands(im.cpu()), 0, 2).numpy()\n",
    "                    for im in patches_norm[:, :3]\n",
    "                ],\n",
    "            )\n",
    "            self.logger.log_image(\n",
    "                \"predictions\", list(no2_normalize.revert(predictions_norm))\n",
    "            )\n",
    "\n",
    "        return no2_loss, no2_mae, lc_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# Instantiate Model\n",
    "unet = UNet(\n",
    "    (13, 64, 128, 256, 512, 1024),\n",
    "    (1024, 512, 256, 128, 64),\n",
    "    SKIP_CONNECTIONS,\n",
    ")\n",
    "\n",
    "model = Model.load_from_checkpoint(\n",
    "    checkpoint_path=CHECKPOINT,\n",
    "    model=unet,\n",
    "    lr=0.000005,\n",
    "    include_lc=LAND_COVER,\n",
    "    lc_loss_weight=0.1,\n",
    "    lc_class_weights=class_weights,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /Users/rubengonzalez/Coding/IMP-2023/lightning_logs\n",
      "/Users/rubengonzalez/miniconda3/envs/imp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 359/359 [03:30<00:00,  1.71it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_lc_loss          2.4139034748077393\n",
      "      test_no2_loss         0.3697924315929413\n",
      "      test_no2_mae          4.9932475090026855\n",
      "      test_no2_mse          49.551387786865234\n",
      "       test_no2_r2          0.4637247920036316\n",
      "     test_total_loss        0.6111828088760376\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_no2_loss': 0.3697924315929413,\n",
       "  'test_no2_mae': 4.9932475090026855,\n",
       "  'test_no2_mse': 49.551387786865234,\n",
       "  'test_no2_r2': 0.4637247920036316,\n",
       "  'test_lc_loss': 2.4139034748077393,\n",
       "  'test_total_loss': 0.6111828088760376}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

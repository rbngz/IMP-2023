{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB NECESSARY CELL\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount drive folder\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone repository with all classes\n",
    "github_pat = input(\"Github Personal Access Token\")\n",
    "!git clone https://{github_pat}@github.com/rbngz/IMP-2023.git\n",
    "\n",
    "# Navigate to repository source\n",
    "%cd /content/IMP-2023\n",
    "\n",
    "# Install additional libraries\n",
    "!pip install -U wandb lightning torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "from torchvision.transforms.v2 import ToImage, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torchsummary import summary\n",
    "\n",
    "from dataset import SentinelDataset\n",
    "from utils import get_dataset_stats\n",
    "from transforms import BandNormalize, TargetNormalize\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for splitting\n",
    "SEED = 42\n",
    "L.seed_everything(42)\n",
    "\n",
    "# File paths\n",
    "SAMPLES_PATH = \"/content/drive/MyDrive/imp-data/samples/samples_S2S5P_2018_2020_eea.csv\"\n",
    "DATA_DIR  = \"/content/drive/MyDrive/imp-data/sentinel-2-eea\"\n",
    "\n",
    "# Hyperparameters\n",
    "N_PATCHES = 4\n",
    "PATCH_SIZE = 128\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "ENCODER_CHANNELS=(12, 64, 128, 256)\n",
    "DECODER_CHANNELS=(256, 128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the samples file\n",
    "samples_df = pd.read_csv(SAMPLES_PATH, index_col=\"idx\")\n",
    "\n",
    "# Remove NA measurements\n",
    "samples_df = samples_df[~samples_df[\"no2\"].isna()]\n",
    "\n",
    "# Random shuffle\n",
    "samples_df = samples_df.sample(frac=1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split samples dataframe to avoid sampling patches across sets\n",
    "df_train, df_val, df_test = np.split(\n",
    "    samples_df, [int(0.7 * len(samples_df)), int(0.85 * len(samples_df))]\n",
    ")\n",
    "print(f\"Train set: {len(df_train)}, Validation set: {len(df_val)}, Test set: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for normalization\n",
    "#stats_train = get_dataset_stats(df_train, DATA_DIR)\n",
    "stats_train = {\n",
    "    \"band_means\": np.array(\n",
    "        [\n",
    "            951.7304533,\n",
    "            887.65642278,\n",
    "            672.43460609,\n",
    "            2309.22885705,\n",
    "            1283.86024167,\n",
    "            1971.3361798,\n",
    "            2221.09058264,\n",
    "            2375.07350295,\n",
    "            2061.81996558,\n",
    "            1556.39466485,\n",
    "            565.24740146,\n",
    "            2376.78314149,\n",
    "        ]\n",
    "    ),\n",
    "    \"band_stds\": np.array(\n",
    "        [\n",
    "            669.08142084,\n",
    "            533.01636366,\n",
    "            490.84281537,\n",
    "            1003.74872792,\n",
    "            591.48734924,\n",
    "            739.32762934,\n",
    "            879.07018525,\n",
    "            945.72499154,\n",
    "            813.62533688,\n",
    "            756.78103222,\n",
    "            325.27273547,\n",
    "            869.95854887,\n",
    "        ]\n",
    "    ),\n",
    "    \"no2_mean\": 20.973578214241755,\n",
    "    \"no2_std\": 11.575741710970245,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalizers for bands and NO2 measurements\n",
    "band_normalize = BandNormalize(stats_train[\"band_means\"], stats_train[\"band_stds\"])\n",
    "no2_normalize = TargetNormalize(stats_train[\"no2_mean\"], stats_train[\"no2_std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transforms for images and measurements\n",
    "transform = Compose([ToImage(), band_normalize])\n",
    "target_transform = no2_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train Dataset\n",
    "dataset_train = SentinelDataset(\n",
    "    df_train,\n",
    "    DATA_DIR,\n",
    "    n_patches=N_PATCHES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    pre_load=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# Create Validation Dataset\n",
    "dataset_val = SentinelDataset(\n",
    "    df_val,\n",
    "    DATA_DIR,\n",
    "    n_patches=N_PATCHES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    pre_load=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    "    )\n",
    "\n",
    "# Create Test Dataset\n",
    "dataset_test = SentinelDataset(\n",
    "    df_test,\n",
    "    DATA_DIR,\n",
    "    n_patches=N_PATCHES,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    pre_load=True,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train dataset: {len(dataset_train)}, Validation dataset: {len(dataset_val)}, Test dataset: {len(dataset_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, persistent_workers=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=12, persistent_workers=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pytorch lightning model\n",
    "\n",
    "\n",
    "class Model(L.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set model\n",
    "        self.model = model\n",
    "\n",
    "        # Set hyperparameters\n",
    "        self.loss = MSELoss()\n",
    "        self.mae = L1Loss()\n",
    "        self.lr = lr\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, mae = self._step(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_mae\", mae)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, mae = self._step(batch, batch_idx == 0)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_mae\", mae)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, mae = self._step(batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_mae\", mae)\n",
    "        return loss\n",
    "\n",
    "    def _step(self, batch, log_predictions=False):\n",
    "        # Unpack batch\n",
    "        patches_norm, measurements_norm, coords = batch\n",
    "\n",
    "        # Get normalized predictions\n",
    "        predictions_norm = self.model(patches_norm)\n",
    "\n",
    "        # Get input and output dimensions\n",
    "        _, _, input_height, input_width = patches_norm.shape\n",
    "        _, _, output_height, output_width = predictions_norm.shape\n",
    "\n",
    "        # Compute prediction offset\n",
    "        offset_height = (input_height - output_height) // 2\n",
    "        offset_width = (input_width - output_width) // 2\n",
    "\n",
    "        # Apply offset to coords\n",
    "        coords[0] -= offset_height\n",
    "        coords[1] -= offset_width\n",
    "\n",
    "        # Extract values in coordinate location\n",
    "        target_values_norm = torch.diag(predictions_norm[:, 0, coords[0], coords[1]])\n",
    "\n",
    "        # Compute loss on normalized data\n",
    "        loss = self.loss(target_values_norm, measurements_norm)\n",
    "\n",
    "        # Compute Mean Absolute Error on unnormalized data\n",
    "        measurements = no2_normalize.revert(measurements_norm)\n",
    "        target_values = no2_normalize.revert(target_values_norm)\n",
    "        mae = self.mae(target_values, measurements)\n",
    "\n",
    "        if log_predictions:\n",
    "            self.logger.log_image(\n",
    "                \"predictions\", list(no2_normalize.revert(predictions_norm)), captions=()\n",
    "            )\n",
    "\n",
    "        return loss, mae\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "unet = UNet(enc_chs=ENCODER_CHANNELS, dec_chs=DECODER_CHANNELS)\n",
    "summary(unet, input_size=(12, PATCH_SIZE, PATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "model = Model(model=unet, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logger for weights & biases\n",
    "wandb_logger = WandbLogger(project=\"IMP-2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer = L.Trainer(max_epochs=100, logger=wandb_logger, log_every_n_steps=400)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
